{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cCZAnNu1AWM4"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U langgraph langchain_anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"ANTHROPIC_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyL3Qio9AYzu",
        "outputId": "6b3447a3-612f-42d6-d1e7-21e99a6dd302"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANTHROPIC_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal\n",
        "from langchain_core.tools import tool\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "\n",
        "class WeatherResponse(BaseModel):\n",
        "    \"\"\"Respond to the user with this\"\"\"\n",
        "\n",
        "    temperature: float = Field(description=\"The temperature in fahrenheit\")\n",
        "    wind_directon: str = Field(\n",
        "        description=\"The direction of the wind in abbreviated form\"\n",
        "    )\n",
        "    wind_speed: float = Field(description=\"The speed of the wind in km/h\")\n",
        "\n",
        "\n",
        "# Inherit 'messages' key from MessagesState, which is a list of chat messages\n",
        "class AgentState(MessagesState):\n",
        "    # Final structured response from the agent\n",
        "    final_response: WeatherResponse\n",
        "\n",
        "\n",
        "@tool\n",
        "def get_weather(city: Literal[\"nyc\", \"sf\"]):\n",
        "    \"\"\"Use this to get weather information.\"\"\"\n",
        "    if city == \"nyc\":\n",
        "        return \"It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees\"\n",
        "    elif city == \"sf\":\n",
        "        return \"It is 75 degrees and sunny in SF, with 3 mph winds in the South-East direction\"\n",
        "    else:\n",
        "        raise AssertionError(\"Unknown city\")\n",
        "\n",
        "\n",
        "tools = [get_weather]\n",
        "\n",
        "model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
        "\n",
        "model_with_tools = model.bind_tools(tools)\n",
        "model_with_structured_output = model.with_structured_output(WeatherResponse)"
      ],
      "metadata": {
        "id": "I_Z9LfuvAe5c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "tools = [get_weather, WeatherResponse]\n",
        "\n",
        "# Force the model to use tools by passing tool_choice=\"any\"\n",
        "model_with_response_tool = model.bind_tools(tools, tool_choice=\"any\")\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: AgentState):\n",
        "    response = model_with_response_tool.invoke(state[\"messages\"])\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# Define the function that responds to the user\n",
        "def respond(state: AgentState):\n",
        "    # Construct the final answer from the arguments of the last tool call\n",
        "    weather_tool_call = state[\"messages\"][-1].tool_calls[0]\n",
        "    response = WeatherResponse(**weather_tool_call[\"args\"])\n",
        "    # Since we're using tool calling to return structured output,\n",
        "    # we need to add  a tool message corresponding to the WeatherResponse tool call,\n",
        "    # This is due to LLM providers' requirement that AI messages with tool calls\n",
        "    # need to be followed by a tool message for each tool call\n",
        "    tool_message = {\n",
        "        \"type\": \"tool\",\n",
        "        \"content\": \"Here is your structured response\",\n",
        "        \"tool_call_id\": weather_tool_call[\"id\"],\n",
        "    }\n",
        "    # We return the final answer\n",
        "    return {\"final_response\": response, \"messages\": [tool_message]}\n",
        "\n",
        "\n",
        "# Define the function that determines whether to continue or not\n",
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    # If there is only one tool call and it is the response tool call we respond to the user\n",
        "    if (\n",
        "        len(last_message.tool_calls) == 1\n",
        "        and last_message.tool_calls[0][\"name\"] == \"WeatherResponse\"\n",
        "    ):\n",
        "        return \"respond\"\n",
        "    # Otherwise we will use the tool node again\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"respond\", respond)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"respond\": \"respond\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "workflow.add_edge(\"respond\", END)\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "VoD00ZTAPRwF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
        "    \"final_response\"\n",
        "]"
      ],
      "metadata": {
        "id": "iVRtgrIrPXzE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxuqwmR3PadB",
        "outputId": "b36fbee6-9566-4e11-9ed8-fc7a6d789903"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(temperature=75.0, wind_directon='SE', wind_speed=3.0)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: AgentState):\n",
        "    response = model_with_tools.invoke(state[\"messages\"])\n",
        "    # We return a list, because this will get added to the existing list\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# Define the function that responds to the user\n",
        "def respond(state: AgentState):\n",
        "    # We call the model with structured output in order to return the same format to the user every time\n",
        "    # state['messages'][-2] is the last ToolMessage in the convo, which we convert to a HumanMessage for the model to use\n",
        "    # We could also pass the entire chat history, but this saves tokens since all we care to structure is the output of the tool\n",
        "    response = model_with_structured_output.invoke(\n",
        "        [HumanMessage(content=state[\"messages\"][-2].content)]\n",
        "    )\n",
        "    # We return the final answer\n",
        "    return {\"final_response\": response}\n",
        "\n",
        "\n",
        "# Define the function that determines whether to continue or not\n",
        "def should_continue(state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    # If there is no function call, then we respond to the user\n",
        "    if not last_message.tool_calls:\n",
        "        return \"respond\"\n",
        "    # Otherwise if there is, we continue\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Define the two nodes we will cycle between\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"respond\", respond)\n",
        "workflow.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Set the entrypoint as `agent`\n",
        "# This means that this node is the first one called\n",
        "workflow.set_entry_point(\"agent\")\n",
        "\n",
        "# We now add a conditional edge\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"respond\": \"respond\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"tools\", \"agent\")\n",
        "workflow.add_edge(\"respond\", END)\n",
        "graph = workflow.compile()"
      ],
      "metadata": {
        "id": "lhtmivalPgEH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
        "    \"final_response\"\n",
        "]"
      ],
      "metadata": {
        "id": "6p4Nm7JsPlAY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XESiPZv8PoCX",
        "outputId": "b47b6d82-6711-40f4-c416-80976c3f80db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WeatherResponse(temperature=75.0, wind_directon='SE', wind_speed=3.0)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "090Z_klYPo3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}